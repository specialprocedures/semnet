{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45795f41",
   "metadata": {},
   "source": [
    "# Load data\n",
    "First we load the data, I've pulled a [generic quotations dataset](https://huggingface.co/datasets/m-ric/english_historical_quotes/) from huggingface, it's downloaded to the repo for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semnet import SemanticNetwork\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import sentence_transformers\n",
    "from cosmograph import cosmo\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"m-ric/english_historical_quotes\", split=\"train\"\n",
    ")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "embedding_model = sentence_transformers.SentenceTransformer(\n",
    "    \"BAAI/bge-base-en-v1.5\"\n",
    ")\n",
    "labels = df[\"quote\"].tolist()\n",
    "embeddings = embedding_model.encode(labels, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "786de70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the network, passing in custom data\n",
    "sem = SemanticNetwork()\n",
    "\n",
    "G = sem.fit_transform(\n",
    "    embeddings=embeddings,\n",
    "    labels=labels,\n",
    "    thresh=0.3,\n",
    "    top_k=20,\n",
    "    node_data={\n",
    "        n: {\"type\": \"quote\", \"author\": df[\"author\"].iloc[n]}\n",
    "        for n, _ in enumerate(labels)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "537e8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality for all nodes\n",
    "import networkx as nx\n",
    "\n",
    "# Drop disconnected nodes\n",
    "subgraphs = list(nx.connected_components(G))\n",
    "largest_subgraph = max(subgraphs, key=len)\n",
    "G = G.subgraph(largest_subgraph).copy()\n",
    "\n",
    "# Calculate degree centrality for all nodes\n",
    "centrality = nx.degree_centrality(G)\n",
    "nx.set_node_attributes(G, centrality, \"degree_centrality\")\n",
    "\n",
    "# Get louvain communities\n",
    "communities = nx.community.louvain_communities(G)\n",
    "community_dict = {}\n",
    "for i, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        community_dict[node] = i\n",
    "nx.set_node_attributes(G, community_dict, \"community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e37a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = sem.to_pandas(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cosmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1f54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459c514e6afd47f397c1ac47d5d250ea",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Cosmograph(background_color=None, components_display_state_mode=None, focused_point_ring_color=None, hovered_pâ€¦"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cosmograph import Cosmograph\n",
    "\n",
    "cosmo = Cosmograph(\n",
    "    points=nodes,\n",
    "    links=edges,\n",
    "    point_id_by=\"node_id\",\n",
    "    point_size_by=\"degree_centrality\",\n",
    "    link_source_by=\"source\",\n",
    "    link_target_by=\"target\",\n",
    "    point_color_by=\"community\",\n",
    "    point_cluster_by=\"community\",\n",
    "    point_label_by=\"label\",\n",
    "    show_cluster_labels=True,\n",
    "    point_include_columns=[\"author\", \"degree_centrality\", \"community\"],\n",
    ")\n",
    "cosmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76dabe",
   "metadata": {},
   "source": [
    "# Building the network\n",
    "Semnet makes constructing an embedding-based network simple. Just bring your own embeddings and pass them to the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semnet import SemanticNetwork\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "docs = df[\"quote\"].tolist()\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c65b7",
   "metadata": {},
   "source": [
    "## API\n",
    "We pass the docs as the labels, and also pass the author as additional data to each node in the network. Passing a single item using `.to_dict()` will record the key as `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc925729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger values for thresh will generate sparser networks with fewer edges and more outliers\n",
    "# I've found good values to be between 1.5 and 5. 3 will get a strong core, but with a large number of outliers\n",
    "sem = SemanticNetwork(thresh=0.3, n_trees=100)\n",
    "sem.fit(embeddings=embeddings, labels=docs, node_data=df[\"author\"].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0bc54",
   "metadata": {},
   "source": [
    "## Under the hood\n",
    "Semnet uses [annoy](https://github.com/spotify/annoy) to perform rapid pair-wise distance calculations across all embeddings in the dataset.\n",
    "\n",
    "The result of this process is an edgelist, which can be used to construct an undirected graph, weighted by the semantic similarity between each record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa2261",
   "metadata": {},
   "source": [
    "# Network analysis of text\n",
    "With our data loaded into a `networkx` object, we now have access to hundreds of graph-based algorithms that can be used to explore, analyse and clean our data.\n",
    "\n",
    "Use cases include:\n",
    "- Outlier detection\n",
    "- Enriching network with non-semantic data\n",
    "- Clustering\n",
    "- Visualisation\n",
    "- Semantic pathways\n",
    "- Deduplication and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1f237",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "Any node that has no edges, has no semantic relationship with any other item in the dataset at the threshold set during training. These records may be considered outliers.\n",
    "\n",
    "We can use networkx to find all connected components in the graph. The demo threshold is pretty high so we'll see a fair few outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3742af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "G = sem.graph_.copy()\n",
    "\n",
    "# Returns generator of sets of connected components\n",
    "connected_components = list(nx.connected_components(G))\n",
    "unconnected_components = [\n",
    "    list(c)[0] for c in connected_components if len(c) == 1\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Number of connected components (groups of 2 or more nodes): {len(connected_components)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of unconnected components (outliers): {len(unconnected_components)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd6f41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d61ccf86",
   "metadata": {},
   "source": [
    "Outliers, in this context represent topics or phrasing that is somewhat unique within the dataset. Exploring the outliers, we can see references to Mexico, Pearl Habour, puppies and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aff0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nodes(subgraph, n=5, seed=12345):\n",
    "    random.seed(seed)\n",
    "    node_candidates = list(subgraph.nodes(data=True))\n",
    "    if len(node_candidates) < n:\n",
    "        n = len(node_candidates)\n",
    "    sample_nodes = random.sample(node_candidates, n)\n",
    "    for idx, data in sample_nodes:\n",
    "        print(f\"{data['name']}, {data['value']}\")\n",
    "\n",
    "\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "large_subgraph = G.subgraph(largest_cc)\n",
    "\n",
    "print(\"Largest\")\n",
    "sample_nodes(large_subgraph, n=10)\n",
    "print()\n",
    "print(\"Outliers\")\n",
    "sample_nodes(G.subgraph(unconnected_components), n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0ca16",
   "metadata": {},
   "source": [
    "How we treat outliers will depend on our use case. As a demonstration, I'm keen at looking at the core of the dataset, getting themes, vibes and relationships rather than trying to classify every node. I drop the outliers and focus on the centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffabc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(G.nodes()), len(unconnected_components))\n",
    "non_ouliers = [n for n in G.nodes() if n not in unconnected_components]\n",
    "G = G.subgraph(non_ouliers)\n",
    "print(f\"Graph after removing outliers has {len(G.nodes())} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed2d65",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99f302",
   "metadata": {},
   "source": [
    "Whilst excellent methods and libraries (e.g., BerTopic) exist for topic modelling on embeddings, the graph structure allows us to use a _relationship_-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e86743",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = nx.community.louvain_communities(G, seed=123, resolution=1.5)\n",
    "for i, comm in enumerate(sorted(communities, key=len, reverse=True)):\n",
    "    print(f\"Community {i+1}, size: {len(comm)}\")\n",
    "    subgraph = G.subgraph(comm)\n",
    "    sample_nodes(subgraph, n=5)\n",
    "    print()\n",
    "\n",
    "    # Label nodes with their community, I put small communities into -1\n",
    "    for node in comm:\n",
    "        if len(comm) > 5:\n",
    "            G.nodes[node][\"community\"] = i + 1\n",
    "        else:\n",
    "            G.nodes[node][\"community\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ea75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmograph import cosmo\n",
    "\n",
    "# Use the new to_pandas method to export the graph\n",
    "nodes, edges = sem.to_pandas(G)\n",
    "\n",
    "# For cosmograph, we need to prepare the data\n",
    "widget = cosmo(\n",
    "    points=nodes,\n",
    "    links=edges,\n",
    "    point_id_by=\"id\",  # Index column\n",
    "    link_source_by=\"source\",\n",
    "    link_target_by=\"target\",\n",
    "    link_strength_by=\"similarity\",\n",
    "    point_color_by=\"community\",  # Color by community\n",
    "    point_cluster_by=\"community\",\n",
    "    show_hovered_point_label=True,\n",
    "    select_point_on_click=True,\n",
    "    point_include_columns=[\"value\"],  # Include author info\n",
    "    point_label_by=\"name\",\n",
    ")\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efccefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest path between two nodes\n",
    "import random\n",
    "\n",
    "\n",
    "for n in range(10):\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "    random.seed(n)\n",
    "    node_a, node_b = random.sample(list(large_subgraph.nodes(data=True)), 2)\n",
    "\n",
    "    all_path = nx.all_simple_paths(\n",
    "        large_subgraph,\n",
    "        source=node_a[0],\n",
    "        target=node_b[0],\n",
    "        cutoff=20,\n",
    "    )\n",
    "    # Find the longest path\n",
    "    sorted_paths = sorted(all_path, key=len, reverse=True)\n",
    "    long_path = sorted_paths[0] if len(sorted_paths) > 0 else None\n",
    "\n",
    "    if long_path is not None:\n",
    "        print(\n",
    "            f\"Long path between:\\n- {node_a[1]['name']}\\n- {node_b[1]['name']}\\n\"\n",
    "        )\n",
    "        for idx in long_path:\n",
    "            print(f\"- {large_subgraph.nodes[idx]['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7fda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = []\n",
    "for idx, community in points.groupby(\"community\"):\n",
    "    top_nodes = community.nlargest(5, \"degree_centrality\")\n",
    "    communities.append(\n",
    "        {\n",
    "            \"community_id\": idx,\n",
    "            \"representative_docs\": top_nodes[\"name\"].values,\n",
    "            \"size\": len(community),\n",
    "        }\n",
    "    )\n",
    "\n",
    "for community in sorted(communities, key=lambda x: x[\"size\"], reverse=True):\n",
    "    print(f\"Community {community['community_id']} (size={community['size']}):\")\n",
    "    for doc in community[\"representative_docs\"]:\n",
    "        print(f\" - {doc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdfd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef3c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest path between two nodes\n",
    "import random\n",
    "\n",
    "\n",
    "def find_shortest_path(graph, source_idx, target_idx):\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source=source_idx, target=target_idx)\n",
    "        return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "\n",
    "largest_component = max(connected_components, key=len)\n",
    "largest_subgraph = reduced_graph.subgraph(largest_component)\n",
    "\n",
    "node_a, node_b = random.sample(list(largest_subgraph.nodes(data=True)), 2)\n",
    "path = find_shortest_path(\n",
    "    largest_subgraph, source_idx=node_a[0], target_idx=node_b[0]\n",
    ")\n",
    "for idx in path:\n",
    "    print(f\"- {largest_subgraph.nodes[idx]['name']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
