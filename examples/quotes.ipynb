{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45795f41",
   "metadata": {},
   "source": [
    "# Load data\n",
    "First we load the data, I've pulled a [generic quotations dataset](https://huggingface.co/datasets/m-ric/english_historical_quotes/) from huggingface, it's downloaded to the repo for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4edd86d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0a51f84ee34d419a9c6a5a72a9e541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/751 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from semnet import SemanticNetwork, to_pandas\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import sentence_transformers\n",
    "\n",
    "dataset = datasets.load_dataset(\n",
    "    \"m-ric/english_historical_quotes\", split=\"train\"\n",
    ")\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "embedding_model = sentence_transformers.SentenceTransformer(\n",
    "    \"BAAI/bge-base-en-v1.5\"\n",
    ")\n",
    "labels = df[\"quote\"].tolist()\n",
    "embeddings = embedding_model.encode(labels, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786de70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the network, passing in custom data\n",
    "sem = SemanticNetwork()\n",
    "\n",
    "G = sem.fit_transform(\n",
    "    embeddings=embeddings,\n",
    "    labels=labels,\n",
    "    thresh=0.3,\n",
    "    top_k=20,\n",
    "    node_data={\n",
    "        n: {\"type\": \"quote\", \"author\": df[\"author\"].iloc[n]}\n",
    "        for n, _ in enumerate(labels)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "537e8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree centrality for all nodes\n",
    "import networkx as nx\n",
    "\n",
    "# Drop disconnected nodes\n",
    "subgraphs = list(nx.connected_components(G))\n",
    "largest_subgraph = max(subgraphs, key=len)\n",
    "G = G.subgraph(largest_subgraph).copy()\n",
    "\n",
    "# Calculate degree centrality for all nodes\n",
    "centrality = nx.degree_centrality(G)\n",
    "nx.set_node_attributes(G, centrality, \"degree_centrality\")\n",
    "\n",
    "# Get louvain communities\n",
    "communities = nx.community.louvain_communities(G)\n",
    "community_dict = {}\n",
    "for i, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        community_dict[node] = i\n",
    "        G.nodes[node][\"community\"] = f\"community_{i}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e37a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges = to_pandas(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for community_12: ['success', 'failure', 'succeed']\n",
      "Top terms for community_33: ['truth', 'lie', 'falsehood']\n",
      "Top terms for community_6: ['faith', 'god', 'believe']\n",
      "Top terms for community_21: ['politics', 'democracy', 'government']\n",
      "Top terms for community_0: ['love', 'life', 'loving']\n",
      "Top terms for community_10: ['knowledge', 'wisdom', 'ignorance']\n",
      "Top terms for community_1: ['hope', 'anger', 'god']\n",
      "Top terms for community_36: ['men', 'man', 'great']\n",
      "Top terms for community_28: ['friendship', 'music', 'friends']\n",
      "Top terms for community_13: ['poetry', 'poet', 'poem']\n",
      "Top terms for community_22: ['freedom', 'free', 'liberty']\n",
      "Top terms for community_9: ['war', 'peace', 'justice']\n",
      "Top terms for community_23: ['art', 'artist', 'nature']\n",
      "Top terms for community_32: ['power', 'religion', 'human']\n",
      "Top terms for community_29: ['dreams', 'best', 'live']\n",
      "Top terms for community_35: ['happiness', 'happy', 'life']\n",
      "Top terms for community_8: ['beauty', 'woman', 'beautiful']\n",
      "Top terms for community_38: ['marriage', 'love', 'married']\n",
      "Top terms for community_7: ['jealousy', 'fear', 'love']\n",
      "Top terms for community_4: ['courage', 'fear', 'man']\n",
      "Top terms for community_34: ['attitude', 'best', 'success']\n",
      "Top terms for community_39: ['life', 'change', 'death']\n",
      "Top terms for community_37: ['future', 'history', 'past']\n",
      "Top terms for community_26: ['education', 'learning', 'experience']\n",
      "Top terms for community_16: ['imagination', 'nature', 'god']\n",
      "Top terms for community_31: ['learning', 'teacher', 'new']\n",
      "Top terms for community_27: ['religion', 'politics', 'man']\n",
      "Top terms for community_14: ['humor', 'funny', 'comedy']\n",
      "Top terms for community_24: ['travel', 'reflects', 'journey']\n",
      "Top terms for community_20: ['government', 'people', 'bible']\n",
      "Top terms for community_11: ['money', 'business', 'people']\n",
      "Top terms for community_2: ['god', 'forgiveness', 'anger']\n",
      "Top terms for community_18: ['strength', 'man', 'does']\n",
      "Top terms for community_19: ['best', 'government', 'healthy']\n",
      "Top terms for community_30: ['death', 'life', 'fear']\n",
      "Top terms for community_5: ['women', 'woman', 'men']\n",
      "Top terms for community_3: ['good', 'respect', 'work']\n",
      "Top terms for community_15: ['architecture', 'space', 'art']\n",
      "Top terms for community_17: ['age', 'matter', 'number']\n",
      "Top terms for community_25: ['great', 'artist', 'born']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTop terms for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommunity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Map onto nodes dataframe\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m nodes[\u001b[33m\"\u001b[39m\u001b[33mtop_terms\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mnodes\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcommunity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_terms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/semnet/.venv/lib/python3.13/site-packages/pandas/core/series.py:4719\u001b[39m, in \u001b[36mSeries.map\u001b[39m\u001b[34m(self, arg, na_action)\u001b[39m\n\u001b[32m   4639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\n\u001b[32m   4640\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4641\u001b[39m     arg: Callable | Mapping | Series,\n\u001b[32m   4642\u001b[39m     na_action: Literal[\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   4643\u001b[39m ) -> Series:\n\u001b[32m   4644\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4645\u001b[39m \u001b[33;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[32m   4646\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4717\u001b[39m \u001b[33;03m    dtype: object\u001b[39;00m\n\u001b[32m   4718\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4719\u001b[39m     new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor(new_values, index=\u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m).__finalize__(\n\u001b[32m   4721\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmap\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4722\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/semnet/.venv/lib/python3.13/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/semnet/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# use tf-idf to get top terms per community\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Apply to whole nodes table\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "X = vectorizer.fit_transform(nodes[\"label\"])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=terms)\n",
    "tfidf_df[\"community\"] = nodes[\"community\"].values\n",
    "\n",
    "top_terms = {}\n",
    "for community in tfidf_df[\"community\"].unique():\n",
    "    community_df = tfidf_df[tfidf_df[\"community\"] == community]\n",
    "    mean_tfidf = community_df.drop(columns=[\"community\"]).mean()\n",
    "    top_terms[community] = (\n",
    "        mean_tfidf.sort_values(ascending=False).head(3).index.tolist()\n",
    "    )\n",
    "\n",
    "for community, terms in top_terms.items():\n",
    "    print(f\"Top terms for {community}: {terms}\")\n",
    "\n",
    "# Map onto nodes dataframe\n",
    "nodes[\"top_terms\"] = (\n",
    "    nodes[\"community\"].map(top_terms).apply(lambda x: \"_\".join(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107b458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "community",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "3fa230db-3eb1-45b8-bdd1-9d060533bc62",
       "rows": [
        [
         "0",
         "success_failure_succeed"
        ],
        [
         "1",
         "truth_lie_falsehood"
        ],
        [
         "2",
         "faith_god_believe"
        ],
        [
         "3",
         "politics_democracy_government"
        ],
        [
         "4",
         "love_life_loving"
        ],
        [
         "5",
         "knowledge_wisdom_ignorance"
        ],
        [
         "6",
         "hope_anger_god"
        ],
        [
         "7",
         "men_man_great"
        ],
        [
         "8",
         "faith_god_believe"
        ],
        [
         "9",
         "faith_god_believe"
        ],
        [
         "10",
         "friendship_music_friends"
        ],
        [
         "11",
         "poetry_poet_poem"
        ],
        [
         "12",
         "poetry_poet_poem"
        ],
        [
         "13",
         "freedom_free_liberty"
        ],
        [
         "14",
         "war_peace_justice"
        ],
        [
         "15",
         "art_artist_nature"
        ],
        [
         "16",
         "war_peace_justice"
        ],
        [
         "17",
         "power_religion_human"
        ],
        [
         "18",
         "dreams_best_live"
        ],
        [
         "19",
         "happiness_happy_life"
        ],
        [
         "20",
         "poetry_poet_poem"
        ],
        [
         "21",
         "poetry_poet_poem"
        ],
        [
         "22",
         "success_failure_succeed"
        ],
        [
         "23",
         "beauty_woman_beautiful"
        ],
        [
         "24",
         "marriage_love_married"
        ],
        [
         "25",
         "jealousy_fear_love"
        ],
        [
         "26",
         "poetry_poet_poem"
        ],
        [
         "27",
         "knowledge_wisdom_ignorance"
        ],
        [
         "28",
         "politics_democracy_government"
        ],
        [
         "29",
         "politics_democracy_government"
        ],
        [
         "30",
         "courage_fear_man"
        ],
        [
         "31",
         "courage_fear_man"
        ],
        [
         "32",
         "attitude_best_success"
        ],
        [
         "33",
         "life_change_death"
        ],
        [
         "34",
         "future_history_past"
        ],
        [
         "35",
         "knowledge_wisdom_ignorance"
        ],
        [
         "36",
         "war_peace_justice"
        ],
        [
         "37",
         "education_learning_experience"
        ],
        [
         "38",
         "marriage_love_married"
        ],
        [
         "39",
         "knowledge_wisdom_ignorance"
        ],
        [
         "40",
         "happiness_happy_life"
        ],
        [
         "41",
         "truth_lie_falsehood"
        ],
        [
         "42",
         "marriage_love_married"
        ],
        [
         "43",
         "politics_democracy_government"
        ],
        [
         "44",
         "art_artist_nature"
        ],
        [
         "45",
         "imagination_nature_god"
        ],
        [
         "46",
         "war_peace_justice"
        ],
        [
         "47",
         "hope_anger_god"
        ],
        [
         "48",
         "knowledge_wisdom_ignorance"
        ],
        [
         "49",
         "happiness_happy_life"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 4786
       }
      },
      "text/plain": [
       "0             success_failure_succeed\n",
       "1                 truth_lie_falsehood\n",
       "2                   faith_god_believe\n",
       "3       politics_democracy_government\n",
       "4                    love_life_loving\n",
       "                    ...              \n",
       "4781             happiness_happy_life\n",
       "4782             happiness_happy_life\n",
       "4783                    men_man_great\n",
       "4784                good_respect_work\n",
       "4785          success_failure_succeed\n",
       "Name: community, Length: 4786, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41c1fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.rename(columns={\"node_id\": \"id\"}).to_csv(\"quotes_nodes.csv\", index=False)\n",
    "edges.to_csv(\"quotes_edges.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1f54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459c514e6afd47f397c1ac47d5d250ea",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Cosmograph(background_color=None, components_display_state_mode=None, focused_point_ring_color=None, hovered_p…"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cosmograph import Cosmograph\n",
    "\n",
    "cosmo = Cosmograph(\n",
    "    points=nodes,\n",
    "    links=edges,\n",
    "    point_id_by=\"node_id\",\n",
    "    point_size_by=\"degree_centrality\",\n",
    "    link_source_by=\"source\",\n",
    "    link_target_by=\"target\",\n",
    "    point_color_by=\"community\",\n",
    "    point_cluster_by=\"community\",\n",
    "    point_label_by=\"label\",\n",
    "    show_cluster_labels=True,\n",
    "    point_include_columns=[\"author\", \"degree_centrality\", \"community\"],\n",
    ")\n",
    "cosmo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76dabe",
   "metadata": {},
   "source": [
    "# Building the network\n",
    "Semnet makes constructing an embedding-based network simple. Just bring your own embeddings and pass them to the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cfd944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semnet import SemanticNetwork\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "docs = df[\"quote\"].tolist()\n",
    "\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8c65b7",
   "metadata": {},
   "source": [
    "## API\n",
    "We pass the docs as the labels, and also pass the author as additional data to each node in the network. Passing a single item using `.to_dict()` will record the key as `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc925729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Larger values for thresh will generate sparser networks with fewer edges and more outliers\n",
    "# I've found good values to be between 1.5 and 5. 3 will get a strong core, but with a large number of outliers\n",
    "sem = SemanticNetwork(thresh=0.3, n_trees=100)\n",
    "sem.fit(embeddings=embeddings, labels=docs, node_data=df[\"author\"].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0bc54",
   "metadata": {},
   "source": [
    "## Under the hood\n",
    "Semnet uses [annoy](https://github.com/spotify/annoy) to perform rapid pair-wise distance calculations across all embeddings in the dataset.\n",
    "\n",
    "The result of this process is an edgelist, which can be used to construct an undirected graph, weighted by the semantic similarity between each record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa2261",
   "metadata": {},
   "source": [
    "# Network analysis of text\n",
    "With our data loaded into a `networkx` object, we now have access to hundreds of graph-based algorithms that can be used to explore, analyse and clean our data.\n",
    "\n",
    "Use cases include:\n",
    "- Outlier detection\n",
    "- Enriching network with non-semantic data\n",
    "- Clustering\n",
    "- Visualisation\n",
    "- Semantic pathways\n",
    "- Deduplication and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1f237",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "Any node that has no edges, has no semantic relationship with any other item in the dataset at the threshold set during training. These records may be considered outliers.\n",
    "\n",
    "We can use networkx to find all connected components in the graph. The demo threshold is pretty high so we'll see a fair few outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3742af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "G = sem.graph_.copy()\n",
    "\n",
    "# Returns generator of sets of connected components\n",
    "connected_components = list(nx.connected_components(G))\n",
    "unconnected_components = [\n",
    "    list(c)[0] for c in connected_components if len(c) == 1\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Number of connected components (groups of 2 or more nodes): {len(connected_components)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of unconnected components (outliers): {len(unconnected_components)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd6f41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d61ccf86",
   "metadata": {},
   "source": [
    "Outliers, in this context represent topics or phrasing that is somewhat unique within the dataset. Exploring the outliers, we can see references to Mexico, Pearl Habour, puppies and such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aff0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_nodes(subgraph, n=5, seed=12345):\n",
    "    random.seed(seed)\n",
    "    node_candidates = list(subgraph.nodes(data=True))\n",
    "    if len(node_candidates) < n:\n",
    "        n = len(node_candidates)\n",
    "    sample_nodes = random.sample(node_candidates, n)\n",
    "    for idx, data in sample_nodes:\n",
    "        print(f\"{data['name']}, {data['value']}\")\n",
    "\n",
    "\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "large_subgraph = G.subgraph(largest_cc)\n",
    "\n",
    "print(\"Largest\")\n",
    "sample_nodes(large_subgraph, n=10)\n",
    "print()\n",
    "print(\"Outliers\")\n",
    "sample_nodes(G.subgraph(unconnected_components), n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0ca16",
   "metadata": {},
   "source": [
    "How we treat outliers will depend on our use case. As a demonstration, I'm keen at looking at the core of the dataset, getting themes, vibes and relationships rather than trying to classify every node. I drop the outliers and focus on the centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffabc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(G.nodes()), len(unconnected_components))\n",
    "non_ouliers = [n for n in G.nodes() if n not in unconnected_components]\n",
    "G = G.subgraph(non_ouliers)\n",
    "print(f\"Graph after removing outliers has {len(G.nodes())} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed2d65",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99f302",
   "metadata": {},
   "source": [
    "Whilst excellent methods and libraries (e.g., BerTopic) exist for topic modelling on embeddings, the graph structure allows us to use a _relationship_-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e86743",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = nx.community.louvain_communities(G, seed=123, resolution=1.5)\n",
    "for i, comm in enumerate(sorted(communities, key=len, reverse=True)):\n",
    "    print(f\"Community {i+1}, size: {len(comm)}\")\n",
    "    subgraph = G.subgraph(comm)\n",
    "    sample_nodes(subgraph, n=5)\n",
    "    print()\n",
    "\n",
    "    # Label nodes with their community, I put small communities into -1\n",
    "    for node in comm:\n",
    "        if len(comm) > 5:\n",
    "            G.nodes[node][\"community\"] = i + 1\n",
    "        else:\n",
    "            G.nodes[node][\"community\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ea75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmograph import cosmo\n",
    "\n",
    "# Use the new to_pandas method to export the graph\n",
    "nodes, edges = sem.to_pandas(G)\n",
    "\n",
    "# For cosmograph, we need to prepare the data\n",
    "widget = cosmo(\n",
    "    points=nodes,\n",
    "    links=edges,\n",
    "    point_id_by=\"id\",  # Index column\n",
    "    link_source_by=\"source\",\n",
    "    link_target_by=\"target\",\n",
    "    link_strength_by=\"similarity\",\n",
    "    point_color_by=\"community\",  # Color by community\n",
    "    point_cluster_by=\"community\",\n",
    "    show_hovered_point_label=True,\n",
    "    select_point_on_click=True,\n",
    "    point_include_columns=[\"value\"],  # Include author info\n",
    "    point_label_by=\"name\",\n",
    ")\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efccefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest path between two nodes\n",
    "import random\n",
    "\n",
    "\n",
    "for n in range(10):\n",
    "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "    random.seed(n)\n",
    "    node_a, node_b = random.sample(list(large_subgraph.nodes(data=True)), 2)\n",
    "\n",
    "    all_path = nx.all_simple_paths(\n",
    "        large_subgraph,\n",
    "        source=node_a[0],\n",
    "        target=node_b[0],\n",
    "        cutoff=20,\n",
    "    )\n",
    "    # Find the longest path\n",
    "    sorted_paths = sorted(all_path, key=len, reverse=True)\n",
    "    long_path = sorted_paths[0] if len(sorted_paths) > 0 else None\n",
    "\n",
    "    if long_path is not None:\n",
    "        print(\n",
    "            f\"Long path between:\\n- {node_a[1]['name']}\\n- {node_b[1]['name']}\\n\"\n",
    "        )\n",
    "        for idx in long_path:\n",
    "            print(f\"- {large_subgraph.nodes[idx]['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7fda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = []\n",
    "for idx, community in points.groupby(\"community\"):\n",
    "    top_nodes = community.nlargest(5, \"degree_centrality\")\n",
    "    communities.append(\n",
    "        {\n",
    "            \"community_id\": idx,\n",
    "            \"representative_docs\": top_nodes[\"name\"].values,\n",
    "            \"size\": len(community),\n",
    "        }\n",
    "    )\n",
    "\n",
    "for community in sorted(communities, key=lambda x: x[\"size\"], reverse=True):\n",
    "    print(f\"Community {community['community_id']} (size={community['size']}):\")\n",
    "    for doc in community[\"representative_docs\"]:\n",
    "        print(f\" - {doc}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdfd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef3c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortest path between two nodes\n",
    "import random\n",
    "\n",
    "\n",
    "def find_shortest_path(graph, source_idx, target_idx):\n",
    "    try:\n",
    "        path = nx.shortest_path(graph, source=source_idx, target=target_idx)\n",
    "        return path\n",
    "    except nx.NetworkXNoPath:\n",
    "        return None\n",
    "\n",
    "\n",
    "largest_component = max(connected_components, key=len)\n",
    "largest_subgraph = reduced_graph.subgraph(largest_component)\n",
    "\n",
    "node_a, node_b = random.sample(list(largest_subgraph.nodes(data=True)), 2)\n",
    "path = find_shortest_path(\n",
    "    largest_subgraph, source_idx=node_a[0], target_idx=node_b[0]\n",
    ")\n",
    "for idx in path:\n",
    "    print(f\"- {largest_subgraph.nodes[idx]['name']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
